{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "import argparse\n",
    "import os \n",
    "import numpy as np\n",
    "import operator\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from constants import *\n",
    "import datasets\n",
    "import evaluation\n",
    "import persistence\n",
    "import learn.interpret\n",
    "import learn.models as models\n",
    "import learn.tools as tools\n",
    "\n",
    "from dataproc import extract_wvs\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '''/home/stack/Documents/w266_project/vaersdata/toy2.csv /home/stack/Documents/w266_project/vocab/vocab.csv full conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --lmbda 0.01 --embed-file /home/stack/Documents/w266_project/vaersdata/train_only.embed --embed-size 100 --gpu'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(Y='full', batch_size=16, bidirectional=None, cell_type='gru', code_emb=None, command='/home/stack/Documents/w266_project/vaersdata/toy2.csv /home/stack/Documents/w266_project/vocab/vocab.csv full conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --lmbda 0.01 --embed-file /home/stack/Documents/w266_project/vaersdata/train_only.embed --embed-size 100 --gpu', criterion='prec_at_8', data_path='/home/stack/Documents/w266_project/vaersdata/toy2.csv', dropout=0.2, embed_file='/home/stack/Documents/w266_project/vaersdata/train_only.embed', embed_size=100, filter_size='10', gpu=True, lmbda=0.01, lr=0.0001, model='conv_attn', n_epochs=200, num_filter_maps=50, patience=10, pool=None, public_model=None, quiet=None, rnn_dim=128, rnn_layers=1, samples=None, stack_filters=None, test_model=None, version='mimic3', vocab='/home/stack/Documents/w266_project/vocab/vocab.csv', weight_decay=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"train a neural network on some clinical documents\")\n",
    "parser.add_argument(\"data_path\", type=str,\n",
    "                    help=\"path to a file containing sorted train data. dev/test splits assumed to have same name format with 'train' replaced by 'dev' and 'test'\")\n",
    "parser.add_argument(\"vocab\", type=str, help=\"path to a file holding vocab word list for discretizing words\")\n",
    "parser.add_argument(\"Y\", type=str, help=\"size of label space\")\n",
    "parser.add_argument(\"model\", type=str, choices=[\"cnn_vanilla\", \"rnn\", \"conv_attn\", \"multi_conv_attn\", \"logreg\", \"saved\"], help=\"model\")\n",
    "parser.add_argument(\"n_epochs\", type=int, help=\"number of epochs to train\")\n",
    "parser.add_argument(\"--embed-file\", type=str, required=False, dest=\"embed_file\",\n",
    "                    help=\"path to a file holding pre-trained embeddings\")\n",
    "parser.add_argument(\"--cell-type\", type=str, choices=[\"lstm\", \"gru\"], help=\"what kind of RNN to use (default: GRU)\", dest='cell_type',\n",
    "                    default='gru')\n",
    "parser.add_argument(\"--rnn-dim\", type=int, required=False, dest=\"rnn_dim\", default=128,\n",
    "                    help=\"size of rnn hidden layer (default: 128)\")\n",
    "parser.add_argument(\"--bidirectional\", dest=\"bidirectional\", action=\"store_const\", required=False, const=True,\n",
    "                    help=\"optional flag for rnn to use a bidirectional model\")\n",
    "parser.add_argument(\"--rnn-layers\", type=int, required=False, dest=\"rnn_layers\", default=1,\n",
    "                    help=\"number of layers for RNN models (default: 1)\")\n",
    "parser.add_argument(\"--embed-size\", type=int, required=False, dest=\"embed_size\", default=100,\n",
    "                    help=\"size of embedding dimension. (default: 100)\")\n",
    "parser.add_argument(\"--filter-size\", type=str, required=False, dest=\"filter_size\", default=4,\n",
    "                    help=\"size of convolution filter to use. (default: 3) For multi_conv_attn, give comma separated integers, e.g. 3,4,5\")\n",
    "parser.add_argument(\"--num-filter-maps\", type=int, required=False, dest=\"num_filter_maps\", default=50,\n",
    "                    help=\"size of conv output (default: 50)\")\n",
    "parser.add_argument(\"--pool\", choices=['max', 'avg'], required=False, dest=\"pool\", help=\"which type of pooling to do (logreg model only)\")\n",
    "parser.add_argument(\"--code-emb\", type=str, required=False, dest=\"code_emb\", \n",
    "                    help=\"point to code embeddings to use for parameter initialization, if applicable\")\n",
    "parser.add_argument(\"--weight-decay\", type=float, required=False, dest=\"weight_decay\", default=0,\n",
    "                    help=\"coefficient for penalizing l2 norm of model weights (default: 0)\")\n",
    "parser.add_argument(\"--lr\", type=float, required=False, dest=\"lr\", default=1e-3,\n",
    "                    help=\"learning rate for Adam optimizer (default=1e-3)\")\n",
    "parser.add_argument(\"--batch-size\", type=int, required=False, dest=\"batch_size\", default=16,\n",
    "                    help=\"size of training batches\")\n",
    "parser.add_argument(\"--dropout\", dest=\"dropout\", type=float, required=False, default=0.5,\n",
    "                    help=\"optional specification of dropout (default: 0.5)\")\n",
    "parser.add_argument(\"--lmbda\", type=float, required=False, dest=\"lmbda\", default=0,\n",
    "                    help=\"hyperparameter to tradeoff BCE loss and similarity embedding loss. defaults to 0, which won't create/use the description embedding module at all. \")\n",
    "parser.add_argument(\"--dataset\", type=str, choices=['mimic2', 'mimic3'], dest=\"version\", default='mimic3', required=False,\n",
    "                    help=\"version of MIMIC in use (default: mimic3)\")\n",
    "parser.add_argument(\"--test-model\", type=str, dest=\"test_model\", required=False, help=\"path to a saved model to load and evaluate\")\n",
    "parser.add_argument(\"--criterion\", type=str, default='f1_micro', required=False, dest=\"criterion\",\n",
    "                    help=\"which metric to use for early stopping (default: f1_micro)\")\n",
    "parser.add_argument(\"--patience\", type=int, default=3, required=False,\n",
    "                    help=\"how many epochs to wait for improved criterion metric before early stopping (default: 3)\")\n",
    "parser.add_argument(\"--gpu\", dest=\"gpu\", action=\"store_const\", required=False, const=True,\n",
    "                    help=\"optional flag to use GPU if available\")\n",
    "parser.add_argument(\"--public-model\", dest=\"public_model\", action=\"store_const\", required=False, const=True,\n",
    "                    help=\"optional flag for testing pre-trained models from the public github\")\n",
    "parser.add_argument(\"--stack-filters\", dest=\"stack_filters\", action=\"store_const\", required=False, const=True,\n",
    "                    help=\"optional flag for multi_conv_attn to instead use concatenated filter outputs, rather than pooling over them\")\n",
    "parser.add_argument(\"--samples\", dest=\"samples\", action=\"store_const\", required=False, const=True,\n",
    "                    help=\"optional flag to save samples of good / bad predictions\")\n",
    "parser.add_argument(\"--quiet\", dest=\"quiet\", action=\"store_const\", required=False, const=True,\n",
    "                    help=\"optional flag not to print so much during training\")\n",
    "args = parser.parse_args(command.split())\n",
    "args.command = command\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lookups...\n",
      "loading pretrained embeddings...\n",
      "adding unk embedding\n",
      "ConvAttnPool(\n",
      "  (embed_drop): Dropout(p=0.2)\n",
      "  (embed): Embedding(59609, 100, padding_idx=0)\n",
      "  (conv): Conv1d (100, 50, kernel_size=(10,), stride=(1,), padding=(5,))\n",
      "  (U): Linear(in_features=50, out_features=330)\n",
      "  (final): Linear(in_features=50, out_features=330)\n",
      "  (desc_embedding): Embedding(59609, 100, padding_idx=0)\n",
      "  (label_conv): Conv1d (100, 50, kernel_size=(10,), stride=(1,), padding=(5,))\n",
      "  (label_fc1): Linear(in_features=50, out_features=50)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def init(args):\n",
    "    \"\"\"\n",
    "        Load data, build model, create optimizer, create vars to hold metrics, etc.\n",
    "    \"\"\"\n",
    "    #need to handle really large text fields\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "    #load vocab and other lookups\n",
    "    desc_embed = args.lmbda > 0\n",
    "    print(\"loading lookups...\")\n",
    "    dicts = datasets.load_lookups(args, desc_embed=desc_embed)\n",
    "\n",
    "    model = tools.pick_model(args, dicts)\n",
    "    print(model)\n",
    "\n",
    "    if not args.test_model:\n",
    "        optimizer = optim.Adam(model.parameters(), weight_decay=args.weight_decay, lr=args.lr)\n",
    "    else:\n",
    "        optimizer = None\n",
    "\n",
    "    params = tools.make_param_dict(args)\n",
    "    \n",
    "    return args, model, optimizer, params, dicts\n",
    "\n",
    "args, model, optimizer, params, dicts = init(args)\n",
    "ind2w, w2ind, ind2c, c2ind = dicts['ind2w'], dicts['w2ind'], dicts['ind2c'], dicts['c2ind']\n",
    "unseen_code_inds = set(ind2c.keys())\n",
    "desc_embed = model.lmbda > 0\n",
    "\n",
    "def train(model, optimizer, Y, epoch, batch_size, data_path, gpu, version, dicts, quiet):\n",
    "    \"\"\"\n",
    "        Training loop.\n",
    "        output: losses for each example for this iteration\n",
    "    \"\"\"\n",
    "    print(\"EPOCH %d\" % epoch)\n",
    "    num_labels = len(dicts['ind2c'])\n",
    "\n",
    "    losses = []\n",
    "    #how often to print some info to stdout\n",
    "    print_every = 25\n",
    "\n",
    "    ind2w, w2ind, ind2c, c2ind = dicts['ind2w'], dicts['w2ind'], dicts['ind2c'], dicts['c2ind']\n",
    "    unseen_code_inds = set(ind2c.keys())\n",
    "    desc_embed = model.lmbda > 0\n",
    "\n",
    "    model.train()\n",
    "    gen = datasets.data_generator(data_path, dicts, batch_size, num_labels, version=version, desc_embed=desc_embed)\n",
    "    \n",
    "    return gen\n",
    "\n",
    "def one_epoch(model, optimizer, Y, epoch, n_epochs, batch_size, data_path, version, testing, dicts, model_dir, \n",
    "              samples, gpu, quiet):\n",
    "    return train(model, optimizer, Y, epoch, batch_size, data_path, gpu, version, dicts, quiet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n"
     ]
    }
   ],
   "source": [
    "gen = one_epoch(model, optimizer, args.Y, 1, args.n_epochs, args.batch_size, args.data_path,\n",
    "          args.version, False, dicts, MODEL_DIR, \n",
    "          args.samples, args.gpu, args.quiet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target, _, code_set, descs = tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[56627, 25065]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for inst in descs[:1]:\n",
    "    inst = np.array([inst[0]])\n",
    "inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([56627, 25065])], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding unk embedding\n"
     ]
    }
   ],
   "source": [
    "W = torch.Tensor(extract_wvs.load_embeddings(args.embed_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(W.size()[0], W.size()[1], padding_idx=0)\n",
    "embed.weight.data = W.clone()\n",
    "W = embed.weight.data\n",
    "desc_embedding = nn.Embedding(W.size()[0], W.size()[1], padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: Index is supposed to be a vector at /opt/conda/conda-bld/pytorch_1512387374934/work/torch/lib/TH/generic/THTensorMath.c:248",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d8e4a8d533a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/camlenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/camlenv/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/camlenv/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: Index is supposed to be a vector at /opt/conda/conda-bld/pytorch_1512387374934/work/torch/lib/TH/generic/THTensorMath.c:248"
     ]
    }
   ],
   "source": [
    "lt = Variable(torch.LongTensor(np.array(inst[0])))\n",
    "d = desc_embedding(lt)\n",
    "d = d.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2094,  0.3963],\n",
       "         [ 0.9777,  0.9178],\n",
       "         [ 0.9537, -0.3299],\n",
       "         [-0.6207,  0.0328],\n",
       "         [ 0.2803, -1.1058],\n",
       "         [-0.8163, -1.5004],\n",
       "         [ 1.4209,  0.8168],\n",
       "         [ 0.2664, -0.3262],\n",
       "         [ 2.2038, -0.8914],\n",
       "         [-0.7598, -0.0435],\n",
       "         [ 0.5017, -0.2119],\n",
       "         [-0.0657, -0.7658],\n",
       "         [-0.3663, -0.3825],\n",
       "         [ 0.0736,  0.1690],\n",
       "         [ 0.4003, -0.7683],\n",
       "         [ 1.9004, -2.0641],\n",
       "         [-0.0515,  0.8780],\n",
       "         [-1.0260, -2.0651],\n",
       "         [-0.2277, -1.8174],\n",
       "         [-0.5912, -0.6174],\n",
       "         [-1.0741, -0.4870],\n",
       "         [-1.7110,  1.4376],\n",
       "         [-0.3606, -2.3478],\n",
       "         [ 1.1292,  1.2159],\n",
       "         [ 0.3338,  1.2606],\n",
       "         [ 0.4167,  1.3694],\n",
       "         [-2.0438, -0.4475],\n",
       "         [-2.0636, -1.6892],\n",
       "         [-0.3210, -0.8554],\n",
       "         [ 0.2676,  0.1202],\n",
       "         [-0.0785, -1.8840],\n",
       "         [ 0.6836, -0.0339],\n",
       "         [-0.1857, -0.0136],\n",
       "         [-0.7712,  0.0796],\n",
       "         [ 1.0841, -1.3705],\n",
       "         [-0.8567,  0.6981],\n",
       "         [-0.3012, -0.0589],\n",
       "         [ 0.3881,  0.3359],\n",
       "         [-0.1102, -0.0971],\n",
       "         [-0.4425,  2.5122],\n",
       "         [ 0.6994,  1.1286],\n",
       "         [-0.1080, -0.0502],\n",
       "         [ 0.9597, -0.3741],\n",
       "         [-1.1382,  1.7799],\n",
       "         [-0.6734, -0.3188],\n",
       "         [-1.5900, -0.2975],\n",
       "         [ 1.2506, -0.0250],\n",
       "         [ 0.6699, -0.7967],\n",
       "         [ 0.1447,  0.3917],\n",
       "         [-0.2723,  2.0167],\n",
       "         [-0.5173, -1.1622],\n",
       "         [ 0.6203, -0.5139],\n",
       "         [ 0.6246, -0.8438],\n",
       "         [ 0.6846, -0.9541],\n",
       "         [ 1.1053, -0.3345],\n",
       "         [ 0.0795,  1.1956],\n",
       "         [ 0.4714,  0.0684],\n",
       "         [ 0.3080, -1.7990],\n",
       "         [-0.6096,  1.1353],\n",
       "         [ 2.1675, -0.9061],\n",
       "         [ 0.0904, -1.3400],\n",
       "         [ 1.5823, -1.6503],\n",
       "         [ 1.0120, -0.1196],\n",
       "         [ 0.6438,  0.6588],\n",
       "         [-0.8310,  0.3078],\n",
       "         [ 1.2842, -1.9814],\n",
       "         [-0.7593,  2.2873],\n",
       "         [ 0.1075, -1.1934],\n",
       "         [ 0.4833, -0.7121],\n",
       "         [ 0.4111,  0.7743],\n",
       "         [-0.5394, -0.7461],\n",
       "         [-0.8091,  0.4533],\n",
       "         [ 0.5725, -0.4772],\n",
       "         [-0.9055,  0.8793],\n",
       "         [ 1.0665,  0.1290],\n",
       "         [-1.3345,  0.2155],\n",
       "         [-0.6362,  0.5598],\n",
       "         [ 2.0150,  1.1643],\n",
       "         [-0.2391,  1.4610],\n",
       "         [-0.7985,  1.4504],\n",
       "         [ 0.1909, -0.4171],\n",
       "         [ 0.1591, -0.6178],\n",
       "         [ 0.1392, -0.3102],\n",
       "         [-0.0672, -0.5364],\n",
       "         [ 0.8217,  0.7623],\n",
       "         [-0.4892,  0.1723],\n",
       "         [ 0.5515,  2.3754],\n",
       "         [ 0.2860, -0.3312],\n",
       "         [-1.3851, -0.6079],\n",
       "         [ 1.3140, -0.1059],\n",
       "         [-0.5073,  0.6231],\n",
       "         [ 1.1661, -0.1904],\n",
       "         [-0.4510, -1.3970],\n",
       "         [-0.3126,  0.8311],\n",
       "         [-0.7988, -0.7771],\n",
       "         [-1.6453,  0.4401],\n",
       "         [-0.8366, -0.2356],\n",
       "         [-0.6162,  0.3802],\n",
       "         [-0.1121, -0.2755],\n",
       "         [ 0.1468, -1.9178]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:camlenv]",
   "language": "python",
   "name": "conda-env-camlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
